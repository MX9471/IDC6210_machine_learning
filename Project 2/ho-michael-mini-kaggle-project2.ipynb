{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8001020,"sourceType":"datasetVersion","datasetId":4711626}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"# preprocessing/data manipulation\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n# classifiers\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:24:52.428224Z","iopub.execute_input":"2024-04-03T01:24:52.428612Z","iopub.status.idle":"2024-04-03T01:24:55.685004Z","shell.execute_reply.started":"2024-04-03T01:24:52.428583Z","shell.execute_reply":"2024-04-03T01:24:55.683623Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Read CSVs","metadata":{}},{"cell_type":"code","source":"test_data = pd.read_csv('/kaggle/input/mini-kaggle-project2-dataset2/test.csv')\n\ntrain_data = pd.read_csv('/kaggle/input/mini-kaggle-project2-dataset2/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:24:55.687480Z","iopub.execute_input":"2024-04-03T01:24:55.688041Z","iopub.status.idle":"2024-04-03T01:24:55.939225Z","shell.execute_reply.started":"2024-04-03T01:24:55.688005Z","shell.execute_reply":"2024-04-03T01:24:55.937737Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Splitting and Pre-processing Dataset\n\nHere, we pre-process the dataset by first converting all objects in all categorical columns to numeric values via mapping dictionary. We then split the dataset, stratify the target variable, as well as add a standard scaler. Missing data imputations are conducted as well, using median values so as to mitigate both outlier and skewed data influence.  ","metadata":{}},{"cell_type":"code","source":"# Create a mapping dictionary for all categorical columns\nmapping_dict = {}\n\nfor col in train_data.columns: \n    if train_data[col].dtype == 'object': \n        mapping = {label: idx for idx, label in enumerate(np.unique(train_data[col]))}\n        mapping_dict[col] = mapping\n\nfor col, mapping in mapping_dict.items():\n    train_data[col] = train_data[col].map(mapping)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:24:55.940688Z","iopub.execute_input":"2024-04-03T01:24:55.941549Z","iopub.status.idle":"2024-04-03T01:24:56.317665Z","shell.execute_reply.started":"2024-04-03T01:24:55.941502Z","shell.execute_reply":"2024-04-03T01:24:56.316747Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Training Data \nX = train_data.drop(columns =['income'], axis = 1)\ny = train_data['income']\n\n# Stratified train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:24:56.318865Z","iopub.execute_input":"2024-04-03T01:24:56.319238Z","iopub.status.idle":"2024-04-03T01:24:56.360510Z","shell.execute_reply.started":"2024-04-03T01:24:56.319205Z","shell.execute_reply":"2024-04-03T01:24:56.359217Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Adding standard scaling to X_train and X_test\nsc = StandardScaler()\n\nX_train_scaled = sc.fit_transform(X_train)\nX_test_scaled = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:24:56.365062Z","iopub.execute_input":"2024-04-03T01:24:56.365474Z","iopub.status.idle":"2024-04-03T01:24:56.397459Z","shell.execute_reply.started":"2024-04-03T01:24:56.365440Z","shell.execute_reply":"2024-04-03T01:24:56.396014Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Handling Missing Data\nimputer = SimpleImputer(strategy='median')\nX_train_imputed = imputer.fit_transform(X_train_scaled)\nX_test_imputed = imputer.transform(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:24:56.399100Z","iopub.execute_input":"2024-04-03T01:24:56.399455Z","iopub.status.idle":"2024-04-03T01:24:56.478652Z","shell.execute_reply.started":"2024-04-03T01:24:56.399426Z","shell.execute_reply":"2024-04-03T01:24:56.477227Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Employing Classification Methods on Training Dataset\n\nAfter pre-processing is complete, we then begin running the training dataset through each classification method. The accuracy score of each will be shown as an output to compare. ","metadata":{}},{"cell_type":"markdown","source":"## Perceptron\n\nIn terms of preprocessing data, the perceptron algorithm does not support regularization outright. Thus, its accuracy score ranks the lowest out of all available methods, but could possibly stand to improve if penalty terms are added to its loss function and subsequently updating its weights. ","metadata":{}},{"cell_type":"code","source":"perceptron = Perceptron()\n\nperceptron.fit(X_train_imputed, y_train)\n\npercep_pred = perceptron.predict(X_test_imputed)\n\nacc = accuracy_score(y_test, percep_pred)\nprint(f'Accuracy: {acc:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:24:56.480280Z","iopub.execute_input":"2024-04-03T01:24:56.480643Z","iopub.status.idle":"2024-04-03T01:24:56.554515Z","shell.execute_reply.started":"2024-04-03T01:24:56.480611Z","shell.execute_reply":"2024-04-03T01:24:56.553008Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Accuracy: 0.684\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Logistic Regression\n\nAs logistic and linear regression typically utilize L1 and L2 regularization, L2 was applied in this case to account for all categorical features - adding a penalty to avoid overfitting. Its accuracy score is a marked improvement over perceptron, yet is not quite high enough to be chosen as the main method. ","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(penalty='l2', C=1.0)\n\nlr.fit(X_train_imputed, y_train)\n\nlr_pred = lr.predict(X_test_imputed)\n\nacc = accuracy_score(y_test, lr_pred)\nprint(f'Accuracy: {acc:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:24:56.557239Z","iopub.execute_input":"2024-04-03T01:24:56.559429Z","iopub.status.idle":"2024-04-03T01:24:56.658385Z","shell.execute_reply.started":"2024-04-03T01:24:56.559368Z","shell.execute_reply":"2024-04-03T01:24:56.656922Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy: 0.825\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## SVM \n\nSVM generates the second highest accuracy score out of all of the classifier methods used. This can most likely be attested to its ability to handle high dimensional spaces - of which there are a good number of features - and its robustness to overfitting. It is only slightly less accurate than our chosen classifier method. ","metadata":{}},{"cell_type":"code","source":"svc = SVC()\n\nsvc.fit(X_train_imputed, y_train)\nsvc_pred = svc.predict(X_test_imputed)\n\nacc = accuracy_score(y_test, svc_pred)\nprint(f'Accuracy: {acc:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:24:56.660630Z","iopub.execute_input":"2024-04-03T01:24:56.661567Z","iopub.status.idle":"2024-04-03T01:25:26.248034Z","shell.execute_reply.started":"2024-04-03T01:24:56.661513Z","shell.execute_reply":"2024-04-03T01:25:26.246759Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Accuracy: 0.847\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Decision Tree\n\nWhile the resulting accuracy score is adequate, it ranks as one of the lower scores overall. This could be due to its tendencies to overfit. ","metadata":{}},{"cell_type":"code","source":"tree = DecisionTreeClassifier()\n\ntree.fit(X_train_imputed, y_train)\ntree_pred = tree.predict(X_test_imputed)\n\nacc = accuracy_score(y_test, tree_pred)\nprint(f'Accuracy: {acc:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:25:26.249622Z","iopub.execute_input":"2024-04-03T01:25:26.250082Z","iopub.status.idle":"2024-04-03T01:25:26.544799Z","shell.execute_reply.started":"2024-04-03T01:25:26.250042Z","shell.execute_reply":"2024-04-03T01:25:26.543449Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Accuracy: 0.811\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## K-Nearest Neighbors\n\nThis was another classifier method which had an adequate accuracy score, but not quite high enough to be considered for the final classifier method. One reason why its accuracy score may not be as high could be due to its sensitivity to irrelevant features. \n","metadata":{}},{"cell_type":"code","source":"knn = KNeighborsClassifier()\n\nknn.fit(X_train_imputed, y_train)\nknn_pred = knn.predict(X_test_imputed)\n\nacc = accuracy_score(y_test, knn_pred)\nprint(f'Accuracy: {acc:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:25:26.546277Z","iopub.execute_input":"2024-04-03T01:25:26.546625Z","iopub.status.idle":"2024-04-03T01:25:32.572216Z","shell.execute_reply.started":"2024-04-03T01:25:26.546597Z","shell.execute_reply":"2024-04-03T01:25:32.571004Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Accuracy: 0.822\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Random Forest\n\n**This classification method was chosen**, as it had the highest accuracy score when compared with the others. Its accuracy score could be attributed to its ability to handle missing values effectively while still maintaining accuracy - especially when a large proportion of the dataset's features are irrelevant. ","metadata":{}},{"cell_type":"code","source":"# Chose Random Forest because it had the highest accuracy score \n\nrf = RandomForestClassifier()\n\nrf.fit(X_train_imputed, y_train)\nrf_pred = rf.predict(X_test_imputed)\n\nacc = accuracy_score(y_test, rf_pred)\nprint(f'Accuracy: {acc:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:25:32.573461Z","iopub.execute_input":"2024-04-03T01:25:32.573780Z","iopub.status.idle":"2024-04-03T01:25:37.993406Z","shell.execute_reply.started":"2024-04-03T01:25:32.573752Z","shell.execute_reply":"2024-04-03T01:25:37.992108Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Accuracy: 0.860\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preparing Classifer Method for Test Dataset\n\nHere, we create the code to pre-process the training and testing dataset to be evaluated by the Random Forest algorithm. The code remains mostly the same as earlier, with the biggest differences being the mapping of the categorical data in the test set, as well as encoding the results of the income level to a new column. ","metadata":{}},{"cell_type":"code","source":"# Create a mapping dictionary for all categorical columns\nmapping_dict = {}\n\n# Creating mapping dictionary for training data\nfor col in train_data.columns: \n    if train_data[col].dtype == 'object': \n        mapping = {label: idx for idx, label in enumerate(np.unique(train_data[col]))}\n        mapping_dict[col] = mapping\n\nfor col, mapping in mapping_dict.items():\n    train_data[col] = train_data[col].map(mapping)\n\n# Creating mapping dictionary for testing data    \nfor col in test_data.columns: \n    if test_data[col].dtype == 'object': \n        mapping = {label: idx for idx, label in enumerate(np.unique(test_data[col]))}\n        mapping_dict[col] = mapping\n\nfor col, mapping in mapping_dict.items():\n    test_data[col] = test_data[col].map(mapping)\n    \n# Training Data \nX_train = train_data.drop(columns =['income'], axis = 1)\ny_train = train_data['income']\n\n# Test Data\nX_test = test_data\n\n# Adding standard scaling to X_train and X_test\nsc = StandardScaler()\n\nX_train_scaled = sc.fit_transform(X_train)\nX_test_scaled = sc.transform(X_test)\n\n# Handling Missing Data\nimputer = SimpleImputer(strategy='median')\nX_train_imputed = imputer.fit_transform(X_train_scaled)\nX_test_imputed = imputer.transform(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:25:37.994903Z","iopub.execute_input":"2024-04-03T01:25:37.995381Z","iopub.status.idle":"2024-04-03T01:25:38.186219Z","shell.execute_reply.started":"2024-04-03T01:25:37.995350Z","shell.execute_reply":"2024-04-03T01:25:38.184739Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating Testing Dataset and Generating CSV File\n\nOnce the data from the training and testing dataset have been pre-processed, the testing dataset is then evaluated with our chosen classifer method, of which the results will be converted to a csv file. ","metadata":{}},{"cell_type":"code","source":"rf2 = RandomForestClassifier()\n\nrf2.fit(X_train_imputed, y_train)\n\nrf2_pred = rf2.predict(X_test_imputed)\nresult = pd.DataFrame({'id': test_data.id, 'income': rf2_pred})\nresult.to_csv('submission.csv', index = False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:25:38.190245Z","iopub.execute_input":"2024-04-03T01:25:38.190644Z","iopub.status.idle":"2024-04-03T01:25:45.293634Z","shell.execute_reply.started":"2024-04-03T01:25:38.190609Z","shell.execute_reply":"2024-04-03T01:25:45.292366Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Final Visualization\n\nOnce the csv file has been generated, a quick visualization of the DataFrame is conducted before final submission.  ","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/working/submission.csv')\nprint(submission)\n\nvalue_counts = submission['income'].value_counts()\nprint(value_counts)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T01:25:45.294836Z","iopub.execute_input":"2024-04-03T01:25:45.295179Z","iopub.status.idle":"2024-04-03T01:25:45.315917Z","shell.execute_reply.started":"2024-04-03T01:25:45.295150Z","shell.execute_reply":"2024-04-03T01:25:45.314670Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"         id  income\n0       392       0\n1      1900       0\n2     24507       0\n3     32817       1\n4     47893       0\n...     ...     ...\n9764  13000       0\n9765  43012       0\n9766  34782       0\n9767  23538       0\n9768  23097       0\n\n[9769 rows x 2 columns]\nincome\n0    7836\n1    1933\nName: count, dtype: int64\n","output_type":"stream"}]}]}